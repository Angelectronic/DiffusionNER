{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7835e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_word_index(words: list, char_idx: int) -> int:\n",
    "    word_starts = []\n",
    "    offset = 0\n",
    "    for word in words:\n",
    "        word_starts.append(offset)\n",
    "        offset += len(word) + 1 \n",
    "\n",
    "    for i in reversed(range(len(word_starts))):\n",
    "        if word_starts[i] <= char_idx:\n",
    "            return i\n",
    "\n",
    "    raise ValueError(\"char_idx nằm ngoài phạm vi câu.\")\n",
    "\n",
    "def process_bert_results(words, ner_results):\n",
    "    word = []\n",
    "    for result in ner_results:\n",
    "        word.append({'type': result['entity_group'], 'start': char_to_word_index(words, result['start']), 'end': char_to_word_index(words, result['end']) + 1, 'source': \"ychenNLP/arabic-ner-ace\"})\n",
    "    return word\n",
    "\n",
    "def process_gliner_results(words, ner_results):\n",
    "    mapping = {\n",
    "        \"Person\": \"PER\",\n",
    "        \"Organization\": \"ORG\",\n",
    "        \"Location\": \"LOC\",\n",
    "        \"Geo-Political Entity\": \"GPE\",\n",
    "        \"Facility\": \"FAC\",\n",
    "        \"Vehicle\": \"VEH\",\n",
    "        \"Weapon\": \"WEA\"\n",
    "    }\n",
    "    word = []\n",
    "    for result in ner_results:\n",
    "        word.append({'type': mapping[result['label']], 'start': char_to_word_index(words, result['start']), 'end': char_to_word_index(words, result['end']) + 1, 'source': \"gliner-community/gliner_small-v2.5\"})\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9efae968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 9988.82it/s]\n",
      "  0%|          | 0/809 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 809/809 [05:14<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from gliner import GLiNER\n",
    "\n",
    "model_gliner = GLiNER.from_pretrained(\"gliner-community/gliner_small-v2.5\", load_tokenizer=True)\n",
    "labels = [\"Person\", \"Organization\", \"Location\", \"Geo-Political Entity\", \"Facility\", \"Vehicle\", \"Weapon\"]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ychenNLP/arabic-ner-ace\")\n",
    "model_bert = AutoModelForTokenClassification.from_pretrained(\"ychenNLP/arabic-ner-ace\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model_bert, tokenizer=tokenizer, device=device, aggregation_strategy=\"simple\")\n",
    "\n",
    "\n",
    "data = json.loads(open('data/datasets/ace2004/ace2004_test_context@2.json').read())\n",
    "for i in tqdm(range(len(data))):\n",
    "    sentence = ' '.join(data[i]['tokens'])\n",
    "    \n",
    "    glidner_predictions = model_gliner.predict_entities(sentence, labels)\n",
    "    bert_predictions = nlp(sentence)\n",
    "\n",
    "    glidner_results = process_gliner_results(data[i]['tokens'], glidner_predictions)\n",
    "    bert_results = process_bert_results(data[i]['tokens'], bert_predictions)\n",
    "    \n",
    "\n",
    "    data[i]['entities_preds'] = glidner_results + bert_results\n",
    "    \n",
    "json.dump(data, open('data/datasets/ace2004/ace2004_test_context@3_pretrained.json', 'w'), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffb5a4",
   "metadata": {},
   "source": [
    "# Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d809a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conf_file(path):\n",
    "    config = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):  # bỏ dòng trống hoặc comment\n",
    "                continue\n",
    "            if '=' in line:\n",
    "                key, value = line.split('=', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                # Chuyển giá trị sang đúng kiểu\n",
    "                if value.lower() == \"true\":\n",
    "                    value = True\n",
    "                elif value.lower() == \"false\":\n",
    "                    value = False\n",
    "                elif value.lower() == \"none\":\n",
    "                    value = None\n",
    "                else:\n",
    "                    try:\n",
    "                        # Chuyển thành số nếu được\n",
    "                        if '.' in value:\n",
    "                            value = float(value)\n",
    "                        else:\n",
    "                            value = int(value)\n",
    "                    except ValueError:\n",
    "                        pass  # giữ nguyên là chuỗi\n",
    "                config[key] = value\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f30c880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_path': 'data/datasets/ace2004/ace2004_train_context@2.json', 'valid_path': 'data/datasets/ace2004/ace2004_dev_context@2.json', 'save_path': 'data/ace2004/', 'save_path_include_iteration': False, 'init_eval': False, 'save_optimizer': False, 'train_log_iter': 1, 'final_eval': False, 'train_batch_size': 2, 'epochs': 2, 'lr': '2e-05', 'lr_warmup': 0.1, 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'match_solver': 'hungarian', 'type_loss': 'celoss', 'nil_weight': -1.0, 'match_boundary_weight': 1.0, 'match_class_weight': 1.0, 'loss_boundary_weight': 1.0, 'loss_class_weight': 1.0, 'match_boundary_type': 'logp', 'repeat_gt_entities': 60, 'eval_every_epochs': 8, 'eval_test': False, 'config': 'configs/ace2004.conf', 'local_rank': -1, 'world_size': -1, 'types_path': 'data/datasets/ace2004/ace2004_types.json', 'tokenizer_path': 'bert-base-cased', 'lowercase': False, 'sampling_processes': 4, 'label': 'ace2004_train', 'log_path': 'data/ace2004/', 'store_predictions': False, 'store_examples': False, 'example_count': None, 'debug': False, 'save_code': True, 'lstm_layers': 2, 'span_attn_layers': 2, 'wo_self_attn': False, 'wo_cross_attn': False, 'split_epoch': 10, 'stage_one_lr_scale': 2.0, 'prop_drop': 0.1, 'soi_pooling': 'lrconcat', 'pos_type': 'sine', 'num_proposals': 60, 'sampling_timesteps': 5, 'beta_schedule': 'cosine', 'timesteps': 1000, 'step_embed_type': 'add', 'sample_dist_type': 'normal', 'scale': 1.0, 'extand_noise_spans': 'concat', 'span_renewal': False, 'step_ensemble': False, 'device_id': 0, 'model_path': 'bert-base-cased', 'model_type': 'diffusionner', 'cpu': False, 'eval_batch_size': 2, 'pool_type': 'max', 'no_overlapping': False, 'no_partial_overlapping': True, 'no_duplicate': False, 'boundary_threshold': 0.0, 'cls_threshold': 0.0, 'entity_threshold': 2.5, 'seed': 488, 'cache_path': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parse dataset 'train': 100%|██████████| 6198/6198 [01:13<00:00, 84.17it/s] \n",
      "Parse dataset 'dev': 100%|██████████| 742/742 [00:08<00:00, 83.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['encodings', 'context_masks', 'seg_encoding', 'context2token_masks', 'token_masks', 'gt_types', 'gt_spans', 'entity_masks', 'meta_doc'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from diffusionner.input_reader import JsonInputReader\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusionner import sampling\n",
    "from diffusionner.entities import Dataset\n",
    "\n",
    "config_path = \"configs/ace2004.conf\"\n",
    "config = read_conf_file(config_path)\n",
    "print(config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['model_path'])\n",
    "logger = logging.getLogger()\n",
    "input_reader = JsonInputReader(config['types_path'], tokenizer, logger, repeat_gt_entities=config['repeat_gt_entities'])\n",
    "input_reader.read({'train': config['train_path'], 'dev': config['valid_path']})\n",
    "\n",
    "train_dataset = input_reader.get_dataset('train')\n",
    "dev_dataset = input_reader.get_dataset('dev')\n",
    "# dev_dataset.switch_mode(Dataset.EVAL_MODE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['train_batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=sampling.collate_fn_padding,\n",
    "    persistent_workers=True\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=config['eval_batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=sampling.collate_fn_padding,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eddfd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertDiffusionNER were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['affine_end.bias', 'affine_end.weight', 'affine_start.bias', 'affine_start.weight', 'alphas_cumprod', 'alphas_cumprod_prev', 'bert.embeddings.position_ids', 'betas', 'downlinear.bias', 'downlinear.weight', 'entity_classifier.classifier.0.bias', 'entity_classifier.classifier.0.weight', 'entity_classifier.classifier.2.bias', 'entity_classifier.classifier.2.weight', 'left_boundary_predictor.boundary_predictor.bias', 'left_boundary_predictor.boundary_predictor.weight', 'left_boundary_predictor.entity_embedding_linear.0.bias', 'left_boundary_predictor.entity_embedding_linear.0.weight', 'left_boundary_predictor.token_embedding_linear.0.bias', 'left_boundary_predictor.token_embedding_linear.0.weight', 'log_one_minus_alphas_cumprod', 'lstm.bias_hh_l0', 'lstm.bias_hh_l0_reverse', 'lstm.bias_hh_l1', 'lstm.bias_hh_l1_reverse', 'lstm.bias_ih_l0', 'lstm.bias_ih_l0_reverse', 'lstm.bias_ih_l1', 'lstm.bias_ih_l1_reverse', 'lstm.weight_hh_l0', 'lstm.weight_hh_l0_reverse', 'lstm.weight_hh_l1', 'lstm.weight_hh_l1_reverse', 'lstm.weight_ih_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_ih_l1', 'lstm.weight_ih_l1_reverse', 'model.embeddings.position_ids', 'p2_loss_weight', 'pos_embeddings.1.bias', 'pos_embeddings.1.weight', 'pos_embeddings.3.bias', 'pos_embeddings.3.weight', 'posterior_log_variance_clipped', 'posterior_mean_coef1', 'posterior_mean_coef2', 'posterior_variance', 'right_boundary_predictor.boundary_predictor.bias', 'right_boundary_predictor.boundary_predictor.weight', 'right_boundary_predictor.entity_embedding_linear.0.bias', 'right_boundary_predictor.entity_embedding_linear.0.weight', 'right_boundary_predictor.token_embedding_linear.0.bias', 'right_boundary_predictor.token_embedding_linear.0.weight', 'spanattention.layers.0.cross_attn.in_proj_bias', 'spanattention.layers.0.cross_attn.in_proj_weight', 'spanattention.layers.0.cross_attn.out_proj.bias', 'spanattention.layers.0.cross_attn.out_proj.weight', 'spanattention.layers.0.linear1.bias', 'spanattention.layers.0.linear1.weight', 'spanattention.layers.0.linear2.bias', 'spanattention.layers.0.linear2.weight', 'spanattention.layers.0.norm1.bias', 'spanattention.layers.0.norm1.weight', 'spanattention.layers.0.norm2.bias', 'spanattention.layers.0.norm2.weight', 'spanattention.layers.0.norm3.bias', 'spanattention.layers.0.norm3.weight', 'spanattention.layers.0.self_attn.in_proj_bias', 'spanattention.layers.0.self_attn.in_proj_weight', 'spanattention.layers.0.self_attn.out_proj.bias', 'spanattention.layers.0.self_attn.out_proj.weight', 'spanattention.layers.1.cross_attn.in_proj_bias', 'spanattention.layers.1.cross_attn.in_proj_weight', 'spanattention.layers.1.cross_attn.out_proj.bias', 'spanattention.layers.1.cross_attn.out_proj.weight', 'spanattention.layers.1.linear1.bias', 'spanattention.layers.1.linear1.weight', 'spanattention.layers.1.linear2.bias', 'spanattention.layers.1.linear2.weight', 'spanattention.layers.1.norm1.bias', 'spanattention.layers.1.norm1.weight', 'spanattention.layers.1.norm2.bias', 'spanattention.layers.1.norm2.weight', 'spanattention.layers.1.norm3.bias', 'spanattention.layers.1.norm3.weight', 'spanattention.layers.1.self_attn.in_proj_bias', 'spanattention.layers.1.self_attn.in_proj_weight', 'spanattention.layers.1.self_attn.out_proj.bias', 'spanattention.layers.1.self_attn.out_proj.weight', 'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod', 'sqrt_recip_alphas_cumprod', 'sqrt_recipm1_alphas_cumprod', 'time_mlp.1.bias', 'time_mlp.1.weight', 'time_mlp.3.bias', 'time_mlp.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\lg\\miniconda3\\envs\\tova\\lib\\site-packages\\transformers\\modeling_utils.py:1575: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pred_logits', 'pred_spans', 'pred_left', 'pred_right'])\n"
     ]
    }
   ],
   "source": [
    "from diffusionner import models\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "from diffusionner.loss import Criterion\n",
    "from diffusionner.evaluator import Evaluator\n",
    "import os\n",
    "\n",
    "class DiffusionNERTrainer(pl.LightningModule):\n",
    "    def __init__(self, config, logger):\n",
    "        super().__init__()\n",
    "        model_class = models.get_model(config['model_type'])\n",
    "        model_config = AutoConfig.from_pretrained(config['model_path'], cache_dir=config['cache_path'])\n",
    "        self.model = model_class.from_pretrained(\n",
    "            config['model_path'],\n",
    "            ignore_mismatched_sizes=True,\n",
    "            # local_files_only = True,\n",
    "            config = model_config,\n",
    "            # Prompt4NER model parameters\n",
    "            entity_type_count=input_reader.entity_type_count,\n",
    "            lstm_layers = config['lstm_layers'],\n",
    "            span_attn_layers = config['span_attn_layers'],\n",
    "            timesteps = config['timesteps'],\n",
    "            beta_schedule = config['beta_schedule'],\n",
    "            sampling_timesteps = config['sampling_timesteps'],\n",
    "            num_proposals = config['num_proposals'],\n",
    "            scale = config['scale'],\n",
    "            extand_noise_spans = config['extand_noise_spans'],\n",
    "            span_renewal = config['span_renewal'],\n",
    "            step_ensemble = config['step_ensemble'],\n",
    "            prop_drop = config['prop_drop'],\n",
    "            soi_pooling = config['soi_pooling'],\n",
    "            pos_type =  config['pos_type'],\n",
    "            step_embed_type = config['step_embed_type'],\n",
    "            sample_dist_type = config['sample_dist_type'],\n",
    "            split_epoch = config['split_epoch'],\n",
    "            pool_type = config['pool_type'],\n",
    "            wo_self_attn = config['wo_self_attn'],\n",
    "            wo_cross_attn = config['wo_cross_attn'])\n",
    "        \n",
    "        self.config = config\n",
    "        self.custom_logger = logger\n",
    "\n",
    "        self.weight_dict = {'loss_ce': config['loss_class_weight'], 'loss_boundary': config['loss_boundary_weight']}\n",
    "        losses = ['labels', 'boundary']\n",
    "        self.criterion = Criterion(input_reader.entity_type_count, self.weight_dict, config['nil_weight'], losses, config['type_loss'], config['match_class_weight'], config['match_boundary_weight'], config['match_boundary_type'], config['match_solver'])\n",
    "        self._predictions_path = os.path.join(config['log_path'], 'predictions_%s_epoch_%s.json')\n",
    "        self._examples_path = os.path.join(config['log_path'], 'examples_%s_%s_epoch_%s.html')\n",
    "\n",
    "    def compute(self, output, gt_types, gt_spans, entity_masks, epoch, batch = None):\n",
    "\n",
    "        gt_types_wo_nil = gt_types.masked_select(entity_masks)\n",
    "        \n",
    "        if len(gt_types_wo_nil) == 0:\n",
    "            return 0.1\n",
    "\n",
    "        sizes = [i.sum() for i in entity_masks]\n",
    "        entity_masks = entity_masks.unsqueeze(2).repeat(1, 1, 2)\n",
    "        spans_wo_nil = gt_spans.masked_select(entity_masks).view(-1, 2)\n",
    "\n",
    "        targets = {\"labels\": gt_types_wo_nil, \"gt_left\":spans_wo_nil[:, 0], \"gt_right\":spans_wo_nil[:, 1], \"sizes\":sizes}\n",
    "\n",
    "        train_loss = []\n",
    "        indices = None\n",
    "\n",
    "        pred_logits, pred_left, pred_right, pred_left, pred_right = output[\"pred_logits\"], output[\"pred_spans\"][:, :, 0], output[\"pred_spans\"][:, :, 1], output[\"pred_left\"], output[\"pred_right\"]\n",
    "\n",
    "        outputs = {\"pred_logits\":pred_logits, \"pred_left\":pred_left, \"pred_right\":pred_right, \"pred_left\":pred_left, \"pred_right\":pred_right, \"token_mask\": batch[\"token_masks\"]}\n",
    "        loss_dict, indices = self.criterion(outputs, targets, epoch, indices = indices)\n",
    "        \n",
    "        train_loss = sum(loss_dict[k] * self.weight_dict[k] for k in loss_dict.keys())\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    def forward(self, batch, is_train = True):\n",
    "        if is_train:\n",
    "            output = self.model(\n",
    "                encodings=batch['encodings'], \n",
    "                context_masks=batch['context_masks'], \n",
    "                seg_encoding = batch['seg_encoding'], \n",
    "                context2token_masks = batch['context2token_masks'], \n",
    "                token_masks = batch['token_masks'],\n",
    "                entity_spans = batch['gt_spans'],\n",
    "                entity_types = batch['gt_types'],\n",
    "                entity_masks = batch['entity_masks'],\n",
    "                meta_doc = batch['meta_doc'], \n",
    "                epoch = self.current_epoch)\n",
    "        else:\n",
    "            output = self.model(\n",
    "                encodings=batch['encodings'], \n",
    "                context_masks=batch['context_masks'], \n",
    "                seg_encoding = batch['seg_encoding'], \n",
    "                context2token_masks = batch['context2token_masks'], \n",
    "                token_masks = batch['token_masks'],\n",
    "                meta_doc = batch['meta_doc'])\n",
    "        return output\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        config = self.config\n",
    "        output = self(batch, is_train = True)\n",
    "        gt_types = batch[\"gt_types\"]\n",
    "        gt_spans = batch[\"gt_spans\"]\n",
    "        entity_masks = batch[\"entity_masks\"]\n",
    "        epoch = self.current_epoch\n",
    "        train_loss = self.compute(output, gt_types, gt_spans, entity_masks, epoch, batch)\n",
    "        self.log(\"train_loss\", train_loss)\n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        config = self.config\n",
    "        output = self(batch, is_train = True)\n",
    "        gt_types = batch[\"gt_types\"]\n",
    "        gt_spans = batch[\"gt_spans\"]\n",
    "        entity_masks = batch[\"entity_masks\"]\n",
    "        epoch = self.current_epoch\n",
    "        val_loss = self.compute(output, gt_types, gt_spans, entity_masks, epoch, batch)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "        self.evaluator.eval_batch(output, batch)\n",
    "        \n",
    "        return val_loss\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.evaluator = Evaluator(dev_dataset, input_reader, tokenizer, self.custom_logger, self.config['no_overlapping'], self.config['no_partial_overlapping'], self.config['no_duplicate'], self._predictions_path, self._examples_path, self.config['example_count'], self.current_epoch, dev_dataset.label,  cls_threshold = self.config['cls_threshold'], boundary_threshold = self.config['boundary_threshold'], entity_threshold = self.config['entity_threshold'], save_prediction = self.config['store_predictions'])\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        ner_eval, ner_loc_eval, ner_cls_eval = self.evaluator.compute_scores()\n",
    "        print(\"NER Evaluation: \", ner_eval)\n",
    "        print(\"NER Location Evaluation: \", ner_loc_eval)\n",
    "        print(\"NER Classification Evaluation: \", ner_cls_eval)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        config = self.config\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=float(config['lr']),\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "\n",
    "        return optimizer\n",
    "    \n",
    "# test model\n",
    "test_model = DiffusionNERTrainer(config, logger)\n",
    "test_model.eval()\n",
    "out = test_model(batch)\n",
    "print(out.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa820a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "model = DiffusionNERTrainer(config, logger)\n",
    "model.train()  \n",
    "tslogger = TensorBoardLogger(config['log_path'], name='ace2004_train')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath=os.path.join(config['log_path'], 'checkpoints'),\n",
    "    filename='ace04-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config['epochs'],\n",
    "    logger=tslogger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "trainer.fit(model, train_loader, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ad47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusionner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
